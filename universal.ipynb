{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb083488",
   "metadata": {},
   "source": [
    "# Universal Pertubations - Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f8959",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9e1210",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplatform\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gfile\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "import os.path\n",
    "from universal.prepare_imagenet_data import preprocess_image_batch, create_imagenet_npy, undo_image_avg\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, getopt\n",
    "import zipfile\n",
    "from timeit import time\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "from universal.universal_pert import universal_perturbation\n",
    "device = '/gpu:0'\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d2c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m497.6/497.6 MB\u001B[0m \u001B[31m632.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:06\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.2.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.9/77.9 KB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m0:01\u001B[0m:01\u001B[0m\n",
      "\u001B[?25hCollecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.5/14.5 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.20 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.22.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow) (57.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.9/224.9 KB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m:01\u001B[0m\n",
      "\u001B[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.7/156.7 KB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=44e2a145536f7a5b270f5425aae877b099bbd5380aae62eaeff4ec529d304447\n",
      "  Stored in directory: /home/waris/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, wrapt, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, markdown, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce4faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ce7fbc",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(y_flat, x, inds):\n",
    "    n = num_classes # Not really necessary, just a quick fix.\n",
    "    loop_vars = [\n",
    "         tf.constant(0, tf.int32),\n",
    "         tf.TensorArray(tf.float32, size=n),\n",
    "    ]\n",
    "    _, jacobian = tf.while_loop(\n",
    "        lambda j,_: j < n,\n",
    "        lambda j,result: (j+1, result.write(j, tf.gradients(y_flat[inds[j]], x))),\n",
    "        loop_vars)\n",
    "    return jacobian.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eea5ae",
   "metadata": {},
   "source": [
    "## Main programm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703133a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_imagenet = '/datasets2/ILSVRC2012/train'\n",
    "path_test_image = 'universal/data/test_img.png'\n",
    "\n",
    "tf_device = tf.device(device)\n",
    "tf_device.__enter__()\n",
    "persisted_sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612e483",
   "metadata": {},
   "source": [
    "### Download inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a173fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model_path = os.path.join('universal', 'data', 'tensorflow_inception_graph.pb')\n",
    "\n",
    "if os.path.isfile(inception_model_path) == 0:\n",
    "    print('Downloading Inception model...')\n",
    "    urlretrieve (\"https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\", os.path.join('python', 'data','inception5h.zip'))\n",
    "    # Unzipping the file\n",
    "    zip_ref = zipfile.ZipFile(os.path.join('python','data', 'inception5h.zip'), 'r')\n",
    "    zip_ref.extract('tensorflow_inception_graph.pb', 'python/data')\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f88627",
   "metadata": {},
   "source": [
    "### Load inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65331ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.path.join(inception_model_path)\n",
    "\n",
    "# Load the Inception model\n",
    "with gfile.FastGFile(model, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    persisted_sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc303a",
   "metadata": {},
   "source": [
    "### Prepare the network & get the universal perturbations vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass to dynamic\n",
    "print(persisted_sess.graph.get_operations())\n",
    "\n",
    "persisted_input = persisted_sess.graph.get_tensor_by_name(\"input:0\")\n",
    "persisted_output = persisted_sess.graph.get_tensor_by_name(\"softmax2_pre_activation:0\")\n",
    "\n",
    "print(persisted_output)\n",
    "\n",
    "\n",
    "print('>> Computing feedforward function...')\n",
    "def f(image_inp):\n",
    "    return persisted_sess.run(persisted_output, feed_dict={persisted_input: np.reshape(image_inp, (-1, 224, 224, 3))})\n",
    "\n",
    "file_perturbation = os.path.join('python', 'data', 'universal.npy')\n",
    "\n",
    "if os.path.isfile(file_perturbation) == 0:\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # TODO: Optimize this construction part!\n",
    "    print('>> Compiling the gradient tensorflow functions. This might take some time...')\n",
    "    y_flat = tf.reshape(persisted_output, (-1,))\n",
    "    inds = tf.placeholder(tf.int32, shape=(num_classes,))\n",
    "    dydx = jacobian(y_flat,persisted_input,inds)\n",
    "    print(dydx)\n",
    "\n",
    "    print('>> Computing gradient function...')\n",
    "    def grad_fs(image_inp, indices): return persisted_sess.run(dydx, feed_dict={persisted_input: image_inp, inds: indices}).squeeze(axis=1)\n",
    "\n",
    "    # Load/Create data\n",
    "    datafile = os.path.join('data', 'imagenet_data.npy')\n",
    "    if os.path.isfile(datafile) == 0:\n",
    "        print('>> Creating pre-processed imagenet data...')\n",
    "        X = create_imagenet_npy(path_train_imagenet)\n",
    "\n",
    "        print('>> Saving the pre-processed imagenet data')\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "        # Save the pre-processed images\n",
    "        # Caution: This can take take a lot of space. Comment this part to discard saving.\n",
    "        np.save(os.path.join('data', 'imagenet_data.npy'), X)\n",
    "\n",
    "    else:\n",
    "        print('>> Pre-processed imagenet data detected')\n",
    "        X = np.load(datafile)\n",
    "        \n",
    "    # Running universal perturbation\n",
    "    v = universal_perturbation(X, f, grad_fs, delta=0.2,num_classes=num_classes)\n",
    "\n",
    "    # Saving the universal perturbation\n",
    "    np.save(os.path.join(file_perturbation), v)\n",
    "\n",
    "else:\n",
    "    print('>> Found a pre-computed universal perturbation! Retrieving it from \", file_perturbation')\n",
    "    v = np.load(file_perturbation)\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.imshow(v[0])\n",
    "plt.title(\"Perturbation vector\")\n",
    "#v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648dae4",
   "metadata": {},
   "source": [
    "### Testing the universal perturbation on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the perturbation on the image\n",
    "labels = open(os.path.join('python', 'data', 'labels.txt'), 'r').read().split('\\n')\n",
    "\n",
    "image_original = preprocess_image_batch([path_test_image], img_size=(256, 256), crop_size=(224, 224), color_mode=\"rgb\")\n",
    "label_original = np.argmax(f(image_original), axis=1).flatten()\n",
    "str_label_original = labels[np.int(label_original)-1].split(',')[0]\n",
    "\n",
    "# Clip the perturbation to make sure images fit in uint8\n",
    "clipped_v = np.clip(undo_image_avg(image_original[0,:,:,:]+v[0,:,:,:]), 0, 255) - np.clip(undo_image_avg(image_original[0,:,:,:]), 0, 255)\n",
    "\n",
    "image_perturbed = image_original + clipped_v[None, :, :, :]\n",
    "label_perturbed = np.argmax(f(image_perturbed), axis=1).flatten()\n",
    "str_label_perturbed = labels[np.int(label_perturbed)-1].split(',')[0]\n",
    "\n",
    "# Show original and perturbed image\n",
    "plt.figure(figsize=(14, 10), dpi=80)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(undo_image_avg(image_original[0, :, :, :]).astype(dtype='uint8'), interpolation=None)\n",
    "plt.title(str_label_original)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(undo_image_avg(image_perturbed[0, :, :, :]).astype(dtype='uint8'), interpolation=None)\n",
    "plt.title(str_label_perturbed)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fadae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_device.__exit__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}